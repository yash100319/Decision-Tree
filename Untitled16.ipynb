{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW0kFtSdYM7y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "           Decision Tree\n",
        "#Theoretical.\n",
        "1. What is a Decision Tree, and how does it work ?\n",
        "- A Decision Tree is a type of supervised machine learning algorithm that is used for classification and regression tasks\n",
        "Start with the entire dataset.\n",
        "\n",
        "Choose the best feature to split the data. The goal is to create the most â€œpureâ€ child nodes. This is based on metrics like:\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Entropy (Information Gain)\n",
        "\n",
        "Variance Reduction (for regression)\n",
        "\n",
        "Split the data into subsets based on the feature value.\n",
        "\n",
        "Repeat the process recursively on each subset.\n",
        "2. What are impurity measures in Decision Trees ?\n",
        "- Impurity measures in decision trees are metrics used to determine how mixed (or impure) the classes are in a node. The goal of a decision tree is to split data in a way that reduces impurity â€” in other words, to create child nodes that are as pure (homogeneous) as possible.\n",
        "\n",
        "3. What is the mathematical formula for Gini Impurity ?\n",
        "- For a node\n",
        "ğ‘¡\n",
        "t with\n",
        "ğ¶\n",
        "C classes, the Gini Impurity is defined as:\n",
        "\n",
        "Gini\n",
        "(\n",
        "ğ‘¡\n",
        ")\n",
        "=\n",
        "1\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ¶\n",
        "ğ‘\n",
        "ğ‘–\n",
        "2\n",
        "Gini(t)=1âˆ’\n",
        "i=1\n",
        "âˆ‘\n",
        "C\n",
        "â€‹\n",
        " p\n",
        "i\n",
        "2\n",
        "â€‹\n",
        "4. What is the mathematical formula for Entropy ?\n",
        "- The Entropy of a node\n",
        "ğ‘¡\n",
        "t, which measures the level of disorder or impurity, is given by:\n",
        "\n",
        "Entropy\n",
        "(\n",
        "ğ‘¡\n",
        ")\n",
        "=\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ¶\n",
        "ğ‘\n",
        "ğ‘–\n",
        "log\n",
        "â¡\n",
        "2\n",
        "(\n",
        "ğ‘\n",
        "ğ‘–\n",
        ")\n",
        "Entropy(t)=âˆ’\n",
        "i=1\n",
        "âˆ‘\n",
        "C\n",
        "â€‹\n",
        " p\n",
        "i\n",
        "â€‹\n",
        " log\n",
        "2\n",
        "â€‹\n",
        " (p\n",
        "i\n",
        "â€‹\n",
        " )\n",
        " 5. * What is Information Gain, and how is it used in Decision Trees ?\n",
        "  - Information Gain (IG) is a metric used to measure how well a feature splits the data in a decision tree. It quantifies the reduction in entropy after a dataset is split on a particular feature.\n",
        "\n",
        "In other words, it tells us how much â€œinformationâ€ a feature gives us about the class â€” or how much uncertainty it removes.\n",
        "6. What is the difference between Gini Impurity and Entropy ?\n",
        "- Information Gain (IG) is a metric used to measure how well a feature splits the data in a decision tree. It quantifies the reduction in entropy after a dataset is split on a particular feature.\n",
        "\n",
        "In other words, it tells us how much â€œinformationâ€ a feature gives us about the class â€” or how much uncertainty it removes.\n",
        "7. What is the mathematical explanation behind Decision Trees ?\n",
        "- A Decision Tree is a tree-structured model used to make predictions by recursively splitting the data into subsets based on feature values. At each node, the tree selects the best feature and threshold that reduces impurity the most.\n",
        "\n",
        "8. What is Pre-Pruning in Decision Trees ?\n",
        "- Pre-pruning (also called early stopping) is a technique used to prevent overfitting in decision trees by stopping the tree growth early, before it perfectly classifies the training data.\n",
        "\n",
        "Instead of growing the full tree and then trimming it (as in post-pruning), pre-pruning halts the split process during tree construction based on certain conditions.\n",
        "9. What is Post-Pruning in Decision Trees ?\n",
        "- Post-pruning (also called cost-complexity pruning or backward pruning) is a technique used to reduce overfitting in decision trees by growing the full tree first and then removing branches that do not contribute significantly to predictive power.\n",
        "\n",
        "Unlike pre-pruning (which stops the tree early), post-pruning allows the tree to grow fully and then simplifies it afterward.\n",
        "10. What is the difference between Pre-Pruning and Post-Pruning ?\n",
        "- Both pre-pruning and post-pruning are techniques used to prevent overfitting in decision trees by controlling their growth. However, they differ in when and how they prune the tree\n",
        "Pre-Pruning:\n",
        "Stop if node has < 10 samples\n",
        "\n",
        "Stop if depth > 5\n",
        "\n",
        "Stop if information gain < 0.01\n",
        "\n",
        "Post-Pruning:\n",
        "Remove subtrees if validation accuracy doesn't decrease\n",
        "\n",
        "Use cost-complexity pruning (minimize error + Î± Ã— tree size)\n",
        "11. What is a Decision Tree Regressor ?\n",
        "- A Decision Tree Regressor is a type of decision tree used for regression tasks â€” that is, predicting continuous numeric values instead of categories\n",
        "The model splits the dataset into regions based on feature values.\n",
        "\n",
        "Each split is chosen to minimize the prediction error (e.g., variance or mean squared error) in the resulting subsets.\n",
        "12. What are the advantages and disadvantages of Decision Trees ?\n",
        "-  Advantages of Decision Trees\n",
        "Easy to Understand and Interpret\n",
        "\n",
        "The tree structure is intuitive and can be visualized.\n",
        "\n",
        "Even non-experts can follow the decision rules.\n",
        "\n",
        "Handles Both Numerical and Categorical Data\n",
        "\n",
        "Works well with mixed data types without extensive preprocessing.\n",
        "\n",
        "Requires Little Data Preparation\n",
        "\n",
        "No need for feature scaling or normalization.\n",
        "\n",
        "Can handle missing values (some implementations)\n",
        " Disadvantages of Decision Trees\n",
        "Prone to Overfitting\n",
        "\n",
        "Trees can become very deep and fit noise in training data.\n",
        "\n",
        "Requires pruning or other regularization techniques.\n",
        "\n",
        "Unstable to Small Changes in Data\n",
        "\n",
        "Slight variations in data can cause big changes in the tree structure.\n",
        "\n",
        "Biased Towards Features with More Levels\n",
        "\n",
        "Features with many distinct values may dominate splits unfairly.\n",
        "\n",
        "Less Accurate than Some Other Methods\n",
        "\n",
        "Often outperformed by ensemble methods like Random Forests or Gradient Boosting.\n",
        "13. * How does a Decision Tree handle missing values\n",
        "- Decision trees can handle missing data during both training and prediction, though the exact approach depends on the implementation.\n",
        "\n",
        "1. During Training\n",
        "Ignore samples with missing values for the splitting feature:\n",
        "When deciding the best split on a feature, some algorithms consider only samples where that feature is present.\n",
        "14. * How does a Decision Tree handle categorical features ?\n",
        "- Decision Trees can naturally work with categorical features, but the way they split on these features depends on the algorithm and implementation.\n",
        "15. What are some real-world applications of Decision Trees/?\n",
        "- Healthcare\n",
        "\n",
        "Disease Diagnosis: Classify patients as having or not having a disease based on symptoms, test results, and medical history.\n",
        "\n",
        "Treatment Recommendations: Decide the best treatment path based on patient data.\n",
        "\n",
        "Predicting Patient Outcomes: Regression trees predict length of hospital stay or readmission risk.\n",
        "\n",
        "Finance\n",
        "\n",
        "Credit Scoring: Approve or reject loan applications by analyzing applicantâ€™s financial history and demographics.\n",
        "\n",
        "Fraud Detection: Identify fraudulent transactions based on patterns in transaction data.\n",
        "\n",
        "Risk Management: Estimate risk levels for investments or insurance underwriting.\n",
        "\n",
        "#Practical\n",
        "1. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy ?\n"
      ],
      "metadata": {
        "id": "sEW0uJQIYPig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "1_UzGU1pcgJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "feature importances\n"
      ],
      "metadata": {
        "id": "Uz6ck1oickdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Initialize Decision Tree Classifier with Gini impurity criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "feature_names = iris.feature_names\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "JxYxO_89cq1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. * Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "model accuracy?\n"
      ],
      "metadata": {
        "id": "vI7QofENcv03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Classifier with Entropy criterion\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy using Entropy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "1lpkJmYvc2qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "Squared Error (MSE)\n"
      ],
      "metadata": {
        "id": "FuoI9cMFc7mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Regressor\n",
        "regressor = Dec\n",
        "\n"
      ],
      "metadata": {
        "id": "tOFTQF6IdAie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. * Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz ?\n"
      ],
      "metadata": {
        "id": "9nv_LkA8dEtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "import pydotplus\n",
        "from IPython.display import Image\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Export tree to DOT format\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Create graph from DOT data\n",
        "graph = graphviz.Source(dot_data)\n",
        "\n",
        "# Display the graph (in Jupyter notebooks)\n",
        "graph.render(\"iris_decision_tree\")  # Saves as PDF and other formats\n",
        "graph.view()  # Opens the saved file\n",
        "\n",
        "# If running in Jupyter, just show inline:\n",
        "# graph  # Uncomment this line to display inline in notebooks\n"
      ],
      "metadata": {
        "id": "bmC6JszxdJe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "accuracy with a fully grown tree"
      ],
      "metadata": {
        "id": "xQCpTrrWdQj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Decision Tree with max depth = 3\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = clf_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# Fully grown Decision Tree (no max depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Accuracy with max depth 3: {accuracy_pruned:.2f}\")\n",
        "print(f\"Accuracy with fully grown tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "jtN4TzZSdaq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "accuracy with a default tree ?\n"
      ],
      "metadata": {
        "id": "1enCpDa5dg6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Decision Tree with min_samples_split=5\n",
        "clf_min_split = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_min_split.fit(X_train, y_train)\n",
        "y_pred_min_split = clf_min_split.predict(X_test)\n",
        "accuracy_min_split = accuracy_score(y_test, y_pred_min_split)\n",
        "\n",
        "# Default Decision Tree\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "print(f\"Accuracy with min_samples_split=5: {accuracy_min_split:.2f}\")\n",
        "print(f\"Accuracy with default parameters: {accuracy_default:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "H_93bD8ddo_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "accuracy with unscaled data\n"
      ],
      "metadata": {
        "id": "aCfhQZMpd4Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ----------- Without Scaling -----------\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# ----------- With Scaling -----------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.pre_\n"
      ],
      "metadata": {
        "id": "OhUauLNMd8Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "classification"
      ],
      "metadata": {
        "id": "np4OXF8LeAl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize One-vs-Rest with Decision Tree Classifier\n",
        "ovr_clf = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "\n",
        "# Train the model\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = ovr_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of OvR Decision Tree Classifier: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "wd8SOSt5eEG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Write a Python program to train a Decision Tree Classifier and display the feature importance scores"
      ],
      "metadata": {
        "id": "FwkhnQD3eIzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = clf.feature_importances_\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Display feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "cJhb8SjAeNPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "with an unrestricted tree*"
      ],
      "metadata": {
        "id": "_MAdmCk1eTNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree Regressor with max_depth=5\n",
        "regressor_restricted = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "regressor_restricted.fit(X_train, y_train)\n",
        "y_pred_restricted = regressor_restricted.predict(X_test)\n",
        "mse_restricted = mean_squared_error(y_test, y_pred_restricted)\n",
        "\n",
        "# Unrestricted Decision Tree Regressor\n",
        "regressor_unrestricted = DecisionTreeRegressor(random_state=42)\n",
        "regressor_unrestricted.fit(X_train, y_train)\n",
        "y_pred_unrestricted = regressor_unrestricted.predict(X_test\n"
      ],
      "metadata": {
        "id": "hq28Uk-heW3D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}